{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "input_layer_num=3\n",
        "hidden_layer_num=3\n",
        "output_layer_num=1\n",
        "learning_rate=0.3\n",
        "t=0.9\n",
        "epochs=10\n",
        "w0b=np.array([0,0,1])\n",
        "w1b=np.array([0,0,1])\n",
        "weights_l1=np.array([[2,1,0],[1,2,2],[0,3,1]])\n",
        "weights_l2=np.array([-1,1,-1])\n",
        "\n",
        "input=np.array([0.2,0.4,0.6])\n",
        "def sigmoid_actiation(zin):\n",
        "  return 1/1+np.exp(-zin)\n",
        "def feedforward(input,weights_l1,weights_l2):\n",
        "  zin=np.dot(weights_l1.T,input)+w0b\n",
        "  z=sigmoid_actiation(zin)\n",
        "  yin=np.dot(z,weights_l2.T)+w1b\n",
        "  y=sigmoid_actiation(yin)\n",
        "  return zin,z,yin,y\n",
        "def error_output_layer(t,y,yin):\n",
        "  return (t-y)*y*(1-y)\n",
        "def ebp_hidden_layer(eo,weights_l2):\n",
        "  return eo*weights_l2.T\n",
        "def error_hidden_layer(ebp,z):\n",
        "  return ebp*z*(1-z)\n",
        "def weight_update_hidden(learning_rate,weights_l2,eh,input,w0b):\n",
        "  return weights_l2+learning_rate*eh*input,w0b+learning_rate*eh\n",
        "def weight_update_output(learning_rate,weights_l1,eo,z,w1b):\n",
        "  return weights_l1+learning_rate*eo*z,w1b+learning_rate*eo\n",
        "for epoch in range(epochs):\n",
        "  zin,z,yin,y=feedforward(input,weights_l1,weights_l2)\n",
        "  eo=error_output_layer(t,y,yin)\n",
        "  ebp=ebp_hidden_layer(eo,weights_l2)\n",
        "  ehl=error_hidden_layer(ebp,z)\n",
        "  weights_l2,w1b=weight_update_output(learning_rate,weights_l2,eo,input,w1b)\n",
        "  weights_l1,w0b=weight_update_hidden(learning_rate,weights_l1,ehl,z,w0b)\n",
        "print(f\"Weights {weights_l2} Bias {w1b} for output_layer\")\n",
        "print(f\"Weights {weights_l1} Bias {w0b} for hidden_layer\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiUUv6XpwEV4",
        "outputId": "aac401b9-f4c6-4239-8ab3-6cf5835c4e56"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights [ 5.37292582 13.74585163  0.30338228] Bias [31.86462908 31.86462908  3.1723038 ] for output_layer\n",
            "Weights [[32.07511002 -1.18051752  0.23444379]\n",
            " [31.07511002 -0.18051752  2.23444379]\n",
            " [30.07511002  0.81948248  1.23444379]] Bias [20.75105843 -2.05552115  1.21494447] for hidden_layer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, input_size):\n",
        "        self.weights = np.random.rand(input_size)\n",
        "        self.bias = np.random.rand()\n",
        "        self.lr = 0.01\n",
        "\n",
        "    def prediction(self, inputs):\n",
        "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
        "        return 1 if weighted_sum > 0 else 0\n",
        "\n",
        "    def train(self, tr_input, labels, epochs):\n",
        "        if len(labels)==2:\n",
        "          self.weights=np.random.rand()\n",
        "        for epoch in range(epochs):\n",
        "            for inputs, label in zip(tr_input, labels):\n",
        "                prediction = self.prediction(inputs)\n",
        "                error = label - prediction\n",
        "                # Update weights and bias\n",
        "                self.weights += self.lr * error * inputs\n",
        "                self.bias += self.lr * error\n",
        "        return self.weights, self.bias\n",
        "\n",
        "    def test(self):\n",
        "        epochs = 10\n",
        "        input_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "        and_output = np.array([0, 0, 0, 1])\n",
        "        or_output = np.array([0, 1, 1, 1])\n",
        "        nor_output = np.array([1, 0, 0, 0])\n",
        "        nand_output = np.array([1, 1, 1, 0])\n",
        "        not_input = np.array([[0], [1]])\n",
        "        not_output = np.array([1, 0])\n",
        "\n",
        "        w1, b1 = self.train(input_data, and_output, epochs)\n",
        "        w2, b2 = self.train(input_data, or_output, epochs)\n",
        "        w3, b3 = self.train(input_data, nand_output, epochs)\n",
        "        w4, b4 = self.train(input_data, nor_output, epochs)\n",
        "        w5, b5 = self.train(not_input, not_output, epochs)\n",
        "\n",
        "        print(f\"Weight {w1} and bias {b1} for AND logic\")\n",
        "        print(f\"Weight {w2} and bias {b2} for OR logic\")\n",
        "        print(f\"Weight {w3} and bias {b3} for NAND logic\")\n",
        "        print(f\"Weight {w4} and bias {b4} for NOR logic\")\n",
        "        print(f\"Weight {w5} and bias {b5} for NOT logic\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Test the perceptron\n",
        "input_size = 2\n",
        "perceptron = Perceptron(input_size)\n",
        "perceptron.test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3v14Rwey3ed",
        "outputId": "3bffe2e6-f012-4913-8df6-a154a7bae99d"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight [0.09623775 0.49620135] and bias -0.1990714941349124 for AND logic\n",
            "Weight [0.09623775 0.49620135] and bias -0.1990714941349124 for OR logic\n",
            "Weight [0.09623775 0.49620135] and bias -0.1990714941349124 for NAND logic\n",
            "Weight [0.09623775 0.49620135] and bias -0.3090714941349125 for NOR logic\n",
            "Weight [0.8684231] and bias -0.3090714941349125 for NOT logic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1wRyNijJ18eb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}